{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjJDYUhU/65OzdOS8d6nql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyarsworld/COSMOS-Ecommerce/blob/master/GradDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_eEf6drrQTlM"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrtuDW2XH3vH",
        "outputId": "96e39609-cee3-4b47-9868-6537401d6ed5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading helper_functions.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "  return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "def relu(Z):\n",
        "  return np.maximum(0, Z)"
      ],
      "metadata": {
        "id": "wyfzVz2BR-OC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initilize_parameters(layer_dims):\n",
        "  state_dict = {}\n",
        "  for l in range(1, len(layer_dims)):\n",
        "    state_dict[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
        "    state_dict[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "  return state_dict"
      ],
      "metadata": {
        "id": "LS_VVU-PQ8YM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(A_prev, W, b, activation):\n",
        "  Z = np.dot(W, A_prev) + b\n",
        "  cache = (A_prev, W, b, Z)\n",
        "  A = sigmoid(Z) if activation == \"sigmoid\" else relu(Z)\n",
        "  return A, cache"
      ],
      "metadata": {
        "id": "r1oOjSxOR4C-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X, state_dict):\n",
        "  L = len(state_dict) // 2\n",
        "  A = X\n",
        "  caches = []\n",
        "  for l in range(1, L):\n",
        "    A, cache = forward(A, state_dict[\"W\" + str(l)], state_dict[\"b\" + str(l)], \"relu\")\n",
        "    caches.append(cache)\n",
        "  A, cache = forward(A, state_dict[\"W\" + str(L)], state_dict[\"b\" + str(L)], \"sigmoid\")\n",
        "  caches.append(cache)\n",
        "\n",
        "  return A, caches"
      ],
      "metadata": {
        "id": "QLzWisqaT1Mo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost(y, yhat):\n",
        "  m = len(y)\n",
        "  return - (1 / m) * np.sum(np.multiply(y, np.log(yhat)) + np.multiply((1 - y), np.log(1 - yhat)))"
      ],
      "metadata": {
        "id": "bDR7onksVIap"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(dA, cache, activation):\n",
        "  A_prev, W, b, Z = cache\n",
        "\n",
        "  if activation == \"sigmoid\":\n",
        "    dZ = np.dot(dA, (A_prev * (1 - A_prev)))\n",
        "  else:\n",
        "    dZ = np.multiply(dA, (Z > 0))\n",
        "\n",
        "  m = A_prev.shape[1]\n",
        "  dW = 1 / m * np.dot(dZ, A_prev.T)\n",
        "  db = 1 / m * np.sum(dZ, axis=1, keepdims = True)\n",
        "  print(W, dZ)\n",
        "  dA_prev = np.dot(W.T, dZ)\n",
        "  return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "eiFl5IbyaaJ9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop(AL, Y, caches):\n",
        "  grads = {}\n",
        "  L = len(caches)\n",
        "  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "  dA_prev, dW, db = backward(dAL, caches[L - 1], \"sigmoid\")\n",
        "  grads[\"dA\" + str(L - 1)] = dA_prev\n",
        "  grads[\"dW\" + str(L)] = dW\n",
        "  grads[\"db\" + str(L)] = db\n",
        "\n",
        "  for l in range(L - 2, -1, -1):\n",
        "    dA_prev, dW, db = backward(grads[\"dA\" + str(l + 1)], caches[l], \"relu\")\n",
        "    grads[\"dA\" + str(l)] = dA_prev\n",
        "    grads[\"dW\" + str(l + 1)] = dW\n",
        "    grads[\"db\" + str(l + 1)] = db\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "3co0kVFqZpB2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(state_dict, grads, lr=0.01):\n",
        "  L = len(layer_dims) - 1\n",
        "  for l in range(1, L + 1):\n",
        "    state_dict[\"W\" + str(l)] = state_dict[\"W\" + str(l)] - lr * grads[\"dW\" + str(l)]\n",
        "    state_dict[\"b\" + str(l)] = state_dict[\"b\" + str(l)] - lr * grads[\"db\" + str(l)]\n",
        "  return state_dict"
      ],
      "metadata": {
        "id": "vd4B4MUjcV-G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0.1, 0.2, -0.1], [0.16, 0.6, -0.3]])\n",
        "Y = np.array([1, 0, 1])\n",
        "layer_dims = [len(X),4,3,1]\n",
        "state_dict = initilize_parameters(layer_dims)\n",
        "\n",
        "# Start\n",
        "A, caches = model(X, state_dict)\n",
        "loss = cost(Y, A)\n",
        "loss\n",
        "grads = backprop(A, Y, caches)\n",
        "state_dict = update(state_dict, grads, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUZ4gBiWdtgl",
        "outputId": "2fa1b242-9747-4e2c-cf3f-1ca7e6707dca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.00644114 -0.00265501  0.00372686]] [[-5.86452165e-05 -2.31316239e-04 -2.56039388e-05]]\n",
            "[[-0.00327448  0.00590931  0.00347938  0.00037465]\n",
            " [-0.00388711 -0.01888726 -0.01387702 -0.00088784]\n",
            " [ 0.00531247 -0.00717529 -0.00323239  0.01669087]] [[-3.77741918e-07 -1.48993976e-06 -0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-0.00000000e+00 -0.00000000e+00 -9.54222819e-08]]\n",
            "[[ 0.00530998 -0.00980276]\n",
            " [-0.01058299  0.03440102]\n",
            " [ 0.00848747 -0.00039595]\n",
            " [ 0.00103299  0.00467294]] [[ 0.00000000e+00  0.00000000e+00 -5.06927621e-10]\n",
            " [-2.23219249e-09 -8.80450963e-09  0.00000000e+00]\n",
            " [-1.31430735e-09 -5.18406532e-09  0.00000000e+00]\n",
            " [-1.41520390e-10 -5.58203488e-10 -0.00000000e+00]]\n"
          ]
        }
      ]
    }
  ]
}